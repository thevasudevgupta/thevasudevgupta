Here is the collection of my open-source work:

**Open Source Contributions to HuggingFace:**
* [Added PyTorch `BigBird-Pegasus` to Transformers](https://github.com/huggingface/transformers/pull/10991)
* [Added PyTorch `BigBird-RoBERTa` to Transformers](https://github.com/huggingface/transformers/pull/10183)
* [Added Flax/Jax `BigBird-RoBERTa` to Transformers](https://github.com/huggingface/transformers/pull/11967)
* [Added script for training `FlaxBigBird` on natural questions to Transformers](https://github.com/huggingface/transformers/pull/12233)
* [Integrated `Microsoft's DeepSpeed` with Accelerate](https://github.com/huggingface/accelerate/pull/82)
* [Added `ModelHubMixin` in Hub](https://github.com/huggingface/huggingface_hub/pull/11)

**Open Source Contributions to TensorFlow (Google):**
* [Exported fine-tuned Wav2Vec2 model to TFHub](https://github.com/tensorflow/tfhub.dev/pull/68)
* [Exported pre-trained Wav2Vec2 model to TFHub](https://github.com/tensorflow/tfhub.dev/pull/65)
* [Added notebook for demonstrating Wav2Vec2 fine-tuning to TFHub](https://github.com/tensorflow/hub/pull/788)
* [Implemented & trained Wav2Vec2 model in TensorFlow](https://github.com/thevasudevgupta/gsoc-wav2vec2/commits?author=thevasudevgupta)

**Publications/Blogs/Posters:**
* [Optimizing adapters for NMT](https://medium.com/offnote-labs/build-a-model-which-can-translate-multiple-indian-languages-to-english-very-efficiently-reduce-55375fb0e1ea)
* [Understanding BigBird's Block Sparse Attention](https://huggingface.co/blog/big-bird)
* [BioBigBird: Leveraging Entire Articles for Biomedical Language Understanding](https://sites.google.com/view/rbcdsairesearchshowcases2023/posters?authuser=0#h.4hipndcyi2vz)

**Talks/Lectures:**
* [BigBird: Transformers on long sequences](https://www.youtube.com/watch?v=G22vNvHmHQ0)
* [Deep Dive into Pre-trained Transformers](https://drive.google.com/file/d/1zUq_decKNqaDkdW6_xZ0lDubo3GNEyij)
* [CUDA Concepts YouTube Series](https://www.youtube.com/playlist?list=PL3xCBlatwrsWQP9ZeCwpmxgHx6BaqXQ1a)

**Tutorials:**
* [`Flax BigBird` evaluation on natural-questions dataset](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/evaluate-flax-natural-questions.ipynb)
* [`PyTorch BigBird` evaluation on natural-questions dataset](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/evaluate-torch-natural-questions.ipynb)
* [`PyTorch BigBirdPegasus` evaluation on PubMed dataset](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
* [How to use BigBird (RoBERTa & Pegasus) for inference](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird-inference.ipynb)
* [Template for fine-tuning a pre-trained Wav2Vec2 SavedModel](https://www.tensorflow.org/hub/tutorials/wav2vec2_saved_model_finetuning)
* [Conversion of TF Wav2Vec2 model to ONNX and compares the latency of ONNX exported model & TF model on CPU](https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/wav2vec2_onnx.ipynb)
* [Wav2Vec2 evaluation (without any padding) on LibriSpeech data](https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/librispeech_evaluation_WER_3.ipynb)
* [Wav2Vec2 SavedModel evaluation (with constant padding upto 246000 length) on LibriSpeech data](https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/librispeech_evaluation_WER_6.ipynb)
* [Small demo of how to use Wav2Vec2 for inference for ASR task](https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/wav2vec2-inference.ipynb)

Please visit my webpage: **https://thevasudevgupta.com/** to know more about my work.
